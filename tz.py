# -*- coding: utf-8 -*-
"""TZ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FB3bjI2BV5cVVtUwfFgJFuaPO1eeFIut
"""

!git clone https://github.com/cog-model/AmbiK-dataset

!pip install -r /content/AmbiK-dataset/requirements.txt

from huggingface_hub import notebook_login
notebook_login()

!cp "/content/AmbiK-dataset/utils/metrics.py" "/content/"

import sys
sys.path.append('/content/AmbiK-dataset/utils/metrics.py')

import metrics
import pandas as pd
import wandb
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from scipy.stats import entropy
from tqdm import tqdm
import wandb
import matplotlib.pyplot as plt
import os
import json
from sentence_transformers import SentenceTransformer
from scipy.spatial.distance import cosine
import torch.nn.functional as F
import re

wandb.login(key='33780493ec2591d3936708b76fe1e229e35139b8')

calibration_data = pd.read_csv("/content/AmbiK-dataset/ambik_dataset/ambik_calib_100.csv")
test_data = pd.read_csv("/content/AmbiK-dataset/ambik_dataset/ambik_test_400.csv")

calibration_data = calibration_data[['answer','question','ambiguous_task', 'ambiguity_type','environment_short']]
test_data = test_data[['answer','question','ambiguous_task', 'ambiguity_type','environment_short']]

# 1. Инициализация в Wandb
wandb.init(project="llm-WhiteBox-KnowNo",
          config={
              "model": "Llama-3.1-8B-Instruct",
              "detection_method": "White-box + KnowNo",
              "n_samples": 4,
              "temperature": 0.7,
              "max_new_tokens": 25
    }
)

wandb.config.update({
    "calibration_samples": len(calibration_data),
    "test_samples": len(test_data),
    "ambiguity_types": list(test_data['ambiguity_type'].unique())
})

# 2. Инициализация модели
model_id = "meta-llama/Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    output_attentions=True,
    output_hidden_states=True,
    attn_implementation="eager",
)

# 3. Адаптированный White-box KnowNo метод
class AmbiguityDetector:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.sbert = SentenceTransformer('all-MiniLM-L6-v2')
        self.model.config.pad_token_id = self.tokenizer.eos_token_id

    def prompt_analyze(self, prompt: str, n_samples=4)->dict:
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        generated_texts = []
        attention_dispersions = []
        hidden_dispersions = []

        max_len = 0

        for _ in range(n_samples):
          with torch.no_grad():
              output = self.model.generate(
                    **inputs,
                    max_new_tokens=30,
                    do_sample=True,
                    temperature=0.7,
                    output_attentions=True,
                    return_dict_in_generate=True,
                    output_hidden_states=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

          generated_texts.append(self.tokenizer.decode(output.sequences[0], skip_special_tokens=True))

          if output.attentions is not None:
                last_layer_attentions = []
                for layer_attn in output.attentions:  # Проход по слоям
                    if isinstance(layer_attn, tuple):
                        layer_attn = layer_attn[0]  # Берем первый элемент кортежа

                    max_len = max(max_len, layer_attn.shape[-1])
                    last_layer_attentions.append(layer_attn[0, :, -1, :])

                padded_attention_tensors = []
                for attn in last_layer_attentions:
                  if attn.shape[-1] < max_len:
                    padding_size = max_len - attn.shape[-1]
                    padded_attn = F.pad(attn, (0, padding_size))
                    padded_attention_tensors.append(padded_attn)
                  else:
                    padded_attention_tensors.append(attn)


                attn_tensor = torch.stack(padded_attention_tensors)
                attention_dispersions.append(torch.std(attn_tensor.mean(dim=1)).item())

            # Обработка hidden states
          if output.hidden_states is not None:
              last_layer = output.hidden_states[-1]
              if isinstance(last_layer, tuple):
                last_hidden = last_layer[0]
              else:
                last_hidden = last_layer

              last_hidden = last_hidden[0, -1, :]
              hidden_dispersions.append(torch.std(last_hidden).item())

        # Вычисляем средние метрики по всем сэмплам
        attention_disp = np.mean(attention_dispersions) if attention_dispersions else 0
        hidden_disp = np.mean(hidden_dispersions) if hidden_dispersions else 0
        cleaned_answers = [clean_llm_answer(ans) for ans in generated_texts]
        return {
            **self.knowno_metrics(cleaned_answers),
            "attention_dispersion": attention_disp,
            "hidden_state_dispersion": hidden_disp,
            'llm_answers': cleaned_answers
        }

    def knowno_metrics(self, texts) ->dict:

        embeddings = self.sbert.encode(texts)
        similarities = []
        for i in range(len(embeddings)):
            for j in range(i+1, len(embeddings)):
                sim = 1 - cosine(embeddings[i], embeddings[j])
                similarities.append(sim)

        avg_similarity = np.mean(similarities) if similarities else 0

        return {
            "knowno_semantic_similarity": avg_similarity,
            "knowno_confidence": float(avg_similarity > 0.3)
        }

    def whitebox_metrics(self, attentions, hiddens)->dict:
        attn_means = torch.stack([attn.mean(dim=0) for attn in attentions])
        hiddens_stack = torch.stack(hiddens)

        attention_dispersion = torch.std(attn_means.mean(dim=1)).item()
        hsd = torch.std(hiddens_stack.mean(dim=0)).item()

        return {
            "attention_dispersion": attention_dispersion,
            "hidden_state_dispersion": hsd
        }

    def detect_ambiguity(self, prompt, threshold=0.3):
        metrics = self.prompt_analyze(prompt)
        # Комбинированное решение на основе всех метрик
        score = 0.3 * (1 - metrics["knowno_semantic_similarity"]) + \
                0.4 * metrics["attention_dispersion"] + \
                0.3 * metrics["hidden_state_dispersion"]
        return score > threshold, metrics

detector = AmbiguityDetector(model, tokenizer)

def convert_numpy_types(obj):
    if isinstance(obj, np.generic):
        return obj.item()
    raise TypeError(f"Object of type {obj.__class__.__name__}is not serializable")

TEST_FILE = "test_results.json"

def clean_llm_answers(answer: str) -> list[str]:
    """Извлекает все ответы модели, удаляя всё техническое (Environment, вопросы, метки)."""
    if not isinstance(answer, str):
        return []

    # 1. Удаляем глобальные технические блоки (Environment, System и т.п.)
    answer = re.sub(
        r'(Enviro?nment:|System:|User:|Пользователь:|Context:|Контекст:|Prompt:|Инструкция:).*?(\n|$)',
        '',
        answer,
        flags=re.IGNORECASE | re.DOTALL
    )

    # 2. Находим все ответы (через Answer/Ответ/>>>/A: и т.д.)
    answer_matches = re.findall(
        r'(?:Answer:|Ответ:|>>>|A:|А:|Output:|Вывод:|Completion:|Результат:)\s*(.*?)(?=\s*(?:Answer:|Ответ:|>>>|A:|А:|$|,|;))',
        answer,
        flags=re.IGNORECASE | re.DOTALL
    )

    # 3. Очищаем каждый ответ от лишнего
    cleaned_answers = []
    for ans in answer_matches:
        # Удаляем нумерацию, маркеры списков
        ans = re.sub(r'^[\d\s]*[\.\-\)\*\+]\s*', '', ans, flags=re.MULTILINE)
        # Удаляем кавычки, если ответ полностью в них
        if (ans.startswith(('"', "'", "«")) and ans.endswith(('"', "'", "»"))):
            ans = ans[1:-1].strip()
        # Финализируем очистку
        ans = re.sub(r'\s+', ' ', ans).strip()
        if ans:
            cleaned_answers.append(ans)

    return cleaned_answers

def run_testing(detector, test_data, optimal_threshold=0.5):
    test_results = []
    # Загрузка предыдущих результатов если есть
    start_idx = len(test_results)
    for idx, row in tqdm(test_data.iloc[start_idx:].iterrows(),
                        total=len(test_data)-start_idx,
                        desc="Тестирование",
                        initial=start_idx):
      try:

        # Полный анализ с обоими методами
        full_question = f"Enviroment: {row['environment_short']}\nQuestion:{row['question']}"
        analysis = detector.prompt_analyze(full_question)

        # Формирование результата
        result = {
            "llm_answers":analysis['llm_answers'],
            "question": row["question"],
            "true_ambiguous": row["ambiguity_type"] != "none",
            "true_ambiguity_type": row["ambiguity_type"],
            **analysis,
            "pred_ambiguous": (analysis["knowno_semantic_similarity"] < optimal_threshold) or
                             (analysis["attention_dispersion"] > 0.2)
        }

        test_results.append(result)

        with open(TEST_FILE, "w") as f:
          json.dump(test_results, f, default=convert_numpy_types)

      except Exception as e:
            print(f"Ошибка при обработке строки {idx}: {e}")
            continue

    # Финальное сохранение
    with open(TEST_FILE, "w") as f:
      json.dump(test_results, f, default=convert_numpy_types)
    test_df = pd.read_json(test_results)
    return test_df

#Тестирование
test_df = run_testing(detector, test_data, optimal_threshold=0.5)

test_df = pd.read_json("test_results.json")
print("Колонки в DataFrame:", test_df.columns.tolist())

wandb.log({
    "final_results": wandb.Table(dataframe=test_df)
    })

def calculate_all_metrics(test_df):
    # Проверка наличия необходимых колонок
    required_cols = ['llm_answers', 'pred_ambiguous', 'llm_answers']
    for col in required_cols:
        if col not in test_df.columns:
            raise ValueError(f"Отсутствует обязательная колонка: {col}")

    # Получаем данные в нужном формате
    llm_answers = test_df['llm_answers'].tolist()
    scores = test_df['knowno_confidence'].tolist() if 'knowno_confidence' in test_df.columns else [None]*len(test_df)
    y_amb_type = test_df['pred_ambiguous'].tolist()
    y_amb_intents = test_df['llm_answers'].tolist()
    y_amb_shortlist = test_df['llm_answers'].tolist() if 'llm_answers' in test_df.columns else [None]*len(test_df)

    # Вычисляем метрики
    batch_metrics = metrics.batch_metric_calculation(
        llm_answers, scores, y_amb_type, y_amb_intents, y_amb_shortlist
    )

    aggregated_metrics = aggreate(batch_metrics)
    _, ambiguity_diff = ambiguity_differentiation(pd.DataFrame(batch_metrics))

    return {
        "batch_metrics": batch_metrics,
        "aggregated_metrics": aggregated_metrics,
        "ambiguity_diff": ambiguity_diff
    }

results = calculate_all_metrics(test_df)
wandb.init(project="ambiguity-analysis")

# Логируем основные метрики
wandb.log({
    "ambiguity_differentiation": results["ambiguity_diff"]
})

# Логируем метрики по типам
for metric in results["aggregated_metrics"]:
    wandb.log({
        f"{metric['ambiguity_type']}/SR": metric['sr_agg'],
        f"{metric['ambiguity_type']}/help_rate": metric['help_rate_agg'],
        f"{metric['ambiguity_type']}/correct_help": metric['amb_detection_agg']
    })

'''
#Калибровка на 100 образцах
RESULTS_FILE = "calibration_results.json"
SAVE_EVERY = 10
# Загрузка предыдущих результатов (если есть)
try:
    with open(RESULTS_FILE, 'r') as file:
        calibration_results = json.load(file)
    print(f"Загружено {len(calibration_results)} предыдущих результатов")
except FileNotFoundError:
    calibration_results = []
    print("Старт новой калибровки")

start_idx = len(calibration_results)
for idx, row in tqdm(calibration_data.iloc[start_idx:].iterrows(),
                    total=len(calibration_data) - start_idx,
                    desc="Калибровка",
                    initial=start_idx):
  if idx % 20 == 0:
        !nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv
  result = {
        "question": row["question"],
        **detector.prompt_analyze(row["question"])
    }

  calibration_results.append(result)

  if idx % SAVE_EVERY == 0:
        # Сохранение на диск
      with open(RESULTS_FILE, "w") as f:
        json.dump(calibration_results, f, default=convert_numpy_types)

        # Логирование в WandB
      current_df = pd.DataFrame(calibration_results[-SAVE_EVERY:])
      wandb.log({
            "progress": idx/len(calibration_data),
            "knowno_similarity": current_df["knowno_semantic_similarity"].mean(),
            "attention_disp": current_df["attention_dispersion"].mean(),
            "batch_samples": wandb.Table(dataframe=current_df)
        })

# Финальное сохранение после завершения
with open(RESULTS_FILE, "w") as f:
    json.dump(calibration_results, f, default=convert_numpy_types)

calibration_df = pd.DataFrame(calibration_results)
wandb.log({
    "final_results": wandb.Table(dataframe=calibration_df),
    "summary_stats": {
        "mean_similarity": calibration_df["knowno_semantic_similarity"].mean(),
        "mean_attention_disp": calibration_df["attention_dispersion"].mean(),
        "accuracy": (calibration_df["ambiguous_task"] ==
                    (calibration_df["knowno_confidence"] < 0.3)).mean()
    }
})

wandb.finish()
'''